{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"CIFAR100_MCBB.ipynb","provenance":[{"file_id":"1XjS7bHZ1qtwiB8HhrJN5QxKP7S_9IObl","timestamp":1626796327289},{"file_id":"1O-qLjtaRYZUaXdi_CaP53NM_4sqXyYpG","timestamp":1624889680305},{"file_id":"1jf7x0sFhk4SJJo_Y6EeiKk1R3BZUEfMH","timestamp":1624439897971},{"file_id":"1jp1ZUm4traW4J54phPKkqAS9-xxMVUVq","timestamp":1624287705254},{"file_id":"19SEmN5wXK1329I6yCiebwx7k_F63_n5x","timestamp":1623965488620}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0343ad4c072846cca4a03d8b36bb0219":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_34921da6de894e82a90bfeeb3aa51535","IPY_MODEL_ce99f83cf40b49b6a971704f3fd0a147"],"layout":"IPY_MODEL_f3f6141259824c2f92d7d2c68d6af321"}},"f3f6141259824c2f92d7d2c68d6af321":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34921da6de894e82a90bfeeb3aa51535":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":" 35%","description_tooltip":null,"layout":"IPY_MODEL_54473311b2e94fb19c003b4318ca1d78","max":170498071,"min":0,"orientation":"horizontal","style":"IPY_MODEL_51a2b5e7b71e4d61bc0b9d2b040d1886","value":58934272}},"ce99f83cf40b49b6a971704f3fd0a147":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_018c7f13e74a4ea59e092b9cc391ca8b","placeholder":"​","style":"IPY_MODEL_3686622b316449d0aab510937d22f0e3","value":" 58934272/170498071 [00:04&lt;00:09, 12150668.34it/s]"}},"51a2b5e7b71e4d61bc0b9d2b040d1886":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"54473311b2e94fb19c003b4318ca1d78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3686622b316449d0aab510937d22f0e3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"018c7f13e74a4ea59e092b9cc391ca8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f4a61209ef54e8aa4ff6543fc567dbd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3136ac2bf4364aef8084b9f750957612","IPY_MODEL_6301fce73d944b5dac137f9e3cb94965"],"layout":"IPY_MODEL_127ed4eb232a45278da2e635958acac0"}},"127ed4eb232a45278da2e635958acac0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3136ac2bf4364aef8084b9f750957612":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe17efadd1904cbeb6ebd815c26914b8","max":64275384,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c39706566a20412c88fb974f4f85668b","value":64275384}},"6301fce73d944b5dac137f9e3cb94965":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_771a2f2de0cc4f9481cc31cd5c9f1da6","placeholder":"​","style":"IPY_MODEL_534dd0f3299746f9ab8bba796fb819b6","value":" 64275456/? [00:04&lt;00:00, 15555166.28it/s]"}},"c39706566a20412c88fb974f4f85668b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"fe17efadd1904cbeb6ebd815c26914b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"534dd0f3299746f9ab8bba796fb819b6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"771a2f2de0cc4f9481cc31cd5c9f1da6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"aJuQa_IUSiEB"},"source":["import os"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7kbAV38tE8Ni"},"source":["os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9ZLXec8jSiEE","outputId":"1b912d17-b41d-4153-a131-02a060334901"},"source":["! echo $CUDA_VISIBLE_DEVICES"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["2\n"]}]},{"cell_type":"code","metadata":{"id":"FoTlM_Fb-Fw2"},"source":["path = '/code/MTBB/CIFAR100'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fN0NyWUESiEG","outputId":"60c4699a-c4db-4397-ff9d-ab253a529ce7"},"source":["pwd"],"execution_count":null,"outputs":[{"data":{"text/plain":["'/code/MTBB/CIFAR100'"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"j5Wedenueous"},"source":["import math\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from tqdm import tqdm, trange\n","from sklearn.metrics import roc_auc_score\n","\n","import pickle as pkl\n","\n","#from torch.utils.tensorboard import SummaryWriter\n","#writer = SummaryWriter(log_dir=\"/content/drive/MyDrive/Colab Notebooks/MoG\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gobe1On_fYzm","executionInfo":{"elapsed":10,"status":"ok","timestamp":1628721483080,"user":{"displayName":"you zhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh32ZF0Azcg0NujEfCqvGOZivVXrQHo4lrJnh5x=s64","userId":"05412124325255519397"},"user_tz":-60},"outputId":"1951ca3b-e8cb-4294-e8e8-b37efe8ad29d"},"source":["DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","LOADER_KWARGS = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else{}\n","print(torch.cuda.is_available()) "],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["True\n"]}]},{"cell_type":"code","metadata":{"id":"BvDEdpeS_WRI"},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z2anPvLv-b9X"},"source":["# Download data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["0343ad4c072846cca4a03d8b36bb0219","f3f6141259824c2f92d7d2c68d6af321","34921da6de894e82a90bfeeb3aa51535","ce99f83cf40b49b6a971704f3fd0a147","51a2b5e7b71e4d61bc0b9d2b040d1886","54473311b2e94fb19c003b4318ca1d78","3686622b316449d0aab510937d22f0e3","018c7f13e74a4ea59e092b9cc391ca8b"]},"id":"1nQTZKvngo5Y","outputId":"1b8f1b9e-0d25-4457-add1-27f5237fda39"},"source":["training_data = datasets.CIFAR10(root='data', train=True, download=True, transform=transforms.ToTensor())\n","test_data = datasets.CIFAR10(root='data', train=False, download=True, transform=transforms.ToTensor())"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0343ad4c072846cca4a03d8b36bb0219","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"]},"metadata":{"tags":[]},"output_type":"display_data"}]},{"cell_type":"code","metadata":{"id":"IaaPtLXFh5fB"},"source":["train_set, val_set = torch.utils.data.random_split(training_data,[40000,10000])\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True, drop_last=True, **LOADER_KWARGS)\n","val_loader = torch.utils.data.DataLoader(val_set, batch_size=128, shuffle=True, drop_last=True, ** LOADER_KWARGS)\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=128, shuffle=True, drop_last=True, **LOADER_KWARGS)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xaaACd17gSGD"},"source":["training_loader = torch.utils.data.DataLoader(training_data, batch_size=128, shuffle=True, drop_last=True, **LOADER_KWARGS)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Nd_DQ96tAcn"},"source":["with open(\"/content/drive/MyDrive/CIFAR/training_loader.pt\", \"wb\") as f:\n","  torch.save(training_loader, f)\n","\n","with open(\"/content/drive/MyDrive/CIFAR/test_loader.pt\", \"wb\") as f:\n","  torch.save(test_loader, f)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bX3ouKQgecyR"},"source":["#OOD dataset: SVHN"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":83,"referenced_widgets":["6f4a61209ef54e8aa4ff6543fc567dbd","127ed4eb232a45278da2e635958acac0","3136ac2bf4364aef8084b9f750957612","6301fce73d944b5dac137f9e3cb94965","c39706566a20412c88fb974f4f85668b","fe17efadd1904cbeb6ebd815c26914b8","534dd0f3299746f9ab8bba796fb819b6","771a2f2de0cc4f9481cc31cd5c9f1da6"]},"id":"m9NaT1Uoejt0","executionInfo":{"elapsed":5046,"status":"ok","timestamp":1628675651123,"user":{"displayName":"you zhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh32ZF0Azcg0NujEfCqvGOZivVXrQHo4lrJnh5x=s64","userId":"05412124325255519397"},"user_tz":-60},"outputId":"9c26f5ed-980b-4d33-ebe2-75b04a5520fc"},"source":["svhn_dataset = datasets.SVHN(root='..data', split='test', transform=transforms.ToTensor(), download=True)\n","svhn_loader = torch.utils.data.DataLoader(svhn_dataset, batch_size=128, drop_last=True, **LOADER_KWARGS)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Using downloaded and verified file: ..data/test_32x32.mat\n"]}]},{"cell_type":"markdown","metadata":{"id":"EGA9e7uO-u9K"},"source":["# Loading data in drive"]},{"cell_type":"code","metadata":{"id":"w99eWD23-uIk"},"source":["with open(os.path.join(path, \"training_loader.pt\"), \"rb\") as f:\n","  training_loader = torch.load(f)\n","\n","with open(os.path.join(path, \"test_loader.pt\"), \"rb\") as f:\n","  test_loader = torch.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rvpwuouNCbjQ","executionInfo":{"elapsed":2,"status":"ok","timestamp":1628721490672,"user":{"displayName":"you zhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh32ZF0Azcg0NujEfCqvGOZivVXrQHo4lrJnh5x=s64","userId":"05412124325255519397"},"user_tz":-60},"outputId":"62368928-b502-4da2-d027-ddcbf310bda9"},"source":["print(training_loader.dataset)\n","print(test_loader.dataset)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset CIFAR100\n","    Number of datapoints: 50000\n","    Root location: .data\n","    Split: Train\n","    StandardTransform\n","Transform: ToTensor()\n","Dataset CIFAR100\n","    Number of datapoints: 10000\n","    Root location: .data\n","    Split: Test\n","    StandardTransform\n","Transform: ToTensor()\n"]}]},{"cell_type":"markdown","metadata":{"id":"e_m9LI9c_cC4"},"source":["# Network"]},{"cell_type":"code","metadata":{"id":"4X-PCes7JhX4"},"source":["eps = 1e-20"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yUnw6UmHjKTt"},"source":["class Gaussian:\n","  def __init__(self, mu, rho):\n","    self.mu = mu\n","    self.rho = rho\n","    self.normal = torch.distributions.Normal(0,1)\n","  \n","  @property\n","  def sigma(self):\n","    return torch.log1p(torch.exp(self.rho))\n","  \n","  def sample(self):\n","    epsilon = self.normal.sample(self.rho.size()).to(DEVICE)\n","    return self.mu + self.sigma * epsilon\n","  \n","  def log_prob(self, input):\n","    return (-math.log(math.sqrt(2 * math.pi)) - torch.log(self.sigma+eps) - ((input - self.mu) ** 2) / (2 * self.sigma ** 2)).sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3uhSN4csp8wj"},"source":["class GaussianPrior:\n","  def __init__(self,mu,sigma):\n","    self.mu = mu\n","    self.sigma = sigma\n","  \n","  def log_prob(self,input):\n","    return (-math.log(math.sqrt(2 * math.pi)) - torch.log(self.sigma) - ((input - self.mu) ** 2) / (2 * self.sigma ** 2)).sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R1xsPOzFlFy2"},"source":["class BayesianLinear(nn.Module):\n","  def __init__(self, n_input, n_output, sigma1):\n","    super().__init__()\n","    self.n_input = n_input\n","    self.n_output = n_output\n","\n","    self.w_mu = nn.Parameter(torch.Tensor(n_output,n_input).normal_(0,math.sqrt(2/n_input)))\n","    self.w_rho = nn.Parameter(torch.Tensor(n_output, n_input).uniform_(-2.253,-2.252))\n","    self.w = Gaussian(self.w_mu, self.w_rho)\n","\n","    self.b_mu = nn.Parameter(torch.Tensor(n_output).normal_(0,math.sqrt(2/n_input)))\n","    self.b_rho = nn.Parameter(torch.Tensor(n_output).uniform_(-2.253,-2.252))\n","    self.b = Gaussian(self.b_mu, self.b_rho)\n","\n","    #Prior: Gaussian\n","    self.w_prior = GaussianPrior(0,sigma1)\n","    self.b_prior = GaussianPrior(0,sigma1)\n","    self.log_prior = 0 \n","    self.log_variational_posterior= 0\n","    self.sigma_mean = 0\n","    self.sigma_std = 0\n","  \n","  def forward(self, input, sample=False):\n","    if self.training or sample:\n","      w = self.w.sample()\n","      b = self.b.sample()\n","    else:\n","      w = self.w_mu\n","      b = self.b_mu\n","    \n","    \n","    self.log_prior = self.w_prior.log_prob(w) + self.b_prior.log_prob(b)\n","    self.log_variational_posterior = self.w.log_prob(w) + self.b.log_prob(b)\n","    \n","    self.sigma_mean = self.w.sigma.mean()\n","    self.sigma_std = self.w.sigma.std()\n","    \n","    \n","    return F.linear(input, w, b)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K3CKFNywXNUO"},"source":["class BayesianConv2D(nn.Module):\n","  def __init__(self, in_channels, out_channels, sigma1, kernel_size=3, stride=1, padding=1):\n","    super().__init__()\n","    self.in_channels = in_channels\n","    self.out_channels = out_channels\n","    self.kernel_size = kernel_size\n","    self.stride = stride\n","    self.padding = padding\n","\n","    self.w_mu = nn.Parameter(torch.Tensor(out_channels,in_channels, kernel_size, kernel_size).normal_(0,math.sqrt(2/(out_channels*in_channels*kernel_size*kernel_size))))\n","    self.w_rho = nn.Parameter(torch.Tensor(out_channels, in_channels, kernel_size, kernel_size).uniform_(-2.253,-2.252))\n","    self.w = Gaussian(self.w_mu, self.w_rho)\n","    # check whether bias is needed\n","\n","    # prior: Gaussian\n","    self.w_prior = GaussianPrior(0,sigma1)\n","    self.log_prior = 0\n","    self.log_variational_posterior = 0\n","\n","  def forward(self, input, sample=False):\n","    if self.training or sample:\n","      w = self.w.sample()\n","    else:\n","      w = self.w_mu\n","    \n","    self.log_prior = self.w_prior.log_prob(w)\n","    self.log_variational_porsterior = self.w.log_prob(w) \n","    return F.conv2d(input, w, bias=None, stride=self.stride, padding=self.padding)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F-u1YEKML7uB"},"source":["def BayesianConv3x3(in_channels, out_channels, sigma1, stride=1):\n","  return BayesianConv2D(in_channels, out_channels, sigma1, kernel_size=3,stride=stride, padding=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6y-8NgAKF19P"},"source":["class TLU(nn.Module):\n","  def __init__(self, num_features):\n","    super().__init__()\n","    self.num_features = num_features\n","    self.tau = nn.parameter.Parameter(torch.Tensor(1,num_features,1,1), requires_grad=True)\n","    self.reset_parameters()\n","  \n","  def reset_parameters(self):\n","    nn.init.kaiming_normal_(self.tau)\n","    #nn.init.zeros_(self.tau)\n","    \n","  def forward(self, x):\n","    return torch.max(x, self.tau)\n","\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5jzg6svOemV3"},"source":["class FRN(nn.Module):\n","  def __init__(self, num_features, eps=1e-6, is_eps_learnable=False):\n","    super().__init__()\n","    self.num_features = num_features\n","    self.init_eps = eps\n","    self.is_eps_learnable = is_eps_learnable\n","\n","    self.weight = nn.parameter.Parameter(torch.Tensor(1, num_features, 1, 1), requires_grad=True)\n","    self.bias = nn.parameter.Parameter(torch.Tensor(1,num_features, 1, 1), requires_grad=True)\n","    if is_eps_learnable:\n","      self.eps = nn.Parameter(torch.Tensor(1))\n","    else:\n","      self.eps = torch.tensor(eps)\n","    self.reset_parameters()\n","  \n","  def reset_parameters(self):\n","    nn.init.kaiming_normal_(self.weight)\n","    nn.init.kaiming_normal_(self.bias)\n","    if self.is_eps_learnable:\n","      nn.init.constant_(self.eps, self.init_eps)\n","\n","  def forward(self,x):\n","    nu2 = x.pow(2).mean(dim=[2,3], keepdim=True)\n","\n","    x = x * torch.rsqrt(nu2 + self.eps.abs())\n","    x = self.weight * x + self.bias\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"igvQvVHVLk4D"},"source":["class ResidualBlock(nn.Module):\n","  def __init__(self, in_channels, out_channels, sigma1, stride=1, downsample=None):\n","    super().__init__()\n","    self.conv1 = BayesianConv3x3(in_channels, out_channels, sigma1, stride)\n","    self.frn1 = nn.BatchNorm2d(out_channels)\n","    self.tlu1 = nn.ReLU(inplace=True)\n","    self.conv2 = BayesianConv3x3(out_channels, out_channels, sigma1)\n","    self.frn2 = nn.BatchNorm2d(out_channels)\n","    self.tlu2 = nn.ReLU(inplace=True)\n","    self.downsample = downsample\n","    self.log_prior = 0\n","    self.log_variational_posterior = 0\n","    self.sigma_mean = 0\n","    self.sigma_std = 0\n","\n","  def forward(self, x):\n","    residual = x\n","    out = self.conv1(x)\n","    out = self.frn1(out)\n","    out = self.tlu1(out)\n","    out = self.conv2(out)\n","    out = self.frn2(out)\n","    if self.downsample:\n","      residual = self.downsample(x)\n","    out += residual\n","    out = self.tlu2(out)\n","    self.log_prior = self.conv1.log_prior + self.conv2.log_prior\n","    self.log_variational_posterior = self.conv1.log_variational_posterior + self.conv2.log_variational_posterior\n","    para = torch.cat((self.conv1.w.sigma.flatten(), self.conv2.w.sigma.flatten()))\n","    self.sigma_mean = para.mean()\n","    self.sigma_std = para.std()\n","    return out\n","  \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bbzny6bHt39o"},"source":["class BayesianResNet14(nn.Module):\n","  def __init__(self, block, sigma1, num_class=10):\n","    super().__init__()\n","    self.num_class = num_class\n","\n","    self.in_channels = 16\n","    self.conv = BayesianConv3x3(3,16, sigma1)\n","    self.frn = nn.BatchNorm2d(16)\n","    self.tlu = nn.ReLU(inplace=True)\n","\n","    self.block1 = ResidualBlock(16,16,sigma1)\n","    self.block2 = ResidualBlock(16,16,sigma1)\n","\n","    downsample1 = nn.Sequential(BayesianConv3x3(16,32,sigma1,2), nn.BatchNorm2d(32))\n","    self.block3 = ResidualBlock(16,32,sigma1,2,downsample1)\n","    self.block4 = ResidualBlock(32,32,sigma1)\n","\n","    downsample2 = nn.Sequential(BayesianConv3x3(32,64,sigma1,2), nn.BatchNorm2d(64))\n","    self.block5 = ResidualBlock(32,64,sigma1,2,downsample2)\n","    self.block6 = ResidualBlock(64,64,sigma1)\n","\n","    self.avg_pool = nn.AvgPool2d(8)\n","    self.fc = BayesianLinear(64, num_class, sigma1)\n","\n","  def forward(self, x, sample=False):\n","    out = self.conv(x)\n","    out = self.frn(out)\n","    out = self.tlu(out)\n","    out = self.block1(out)\n","    out = self.block2(out)\n","    out = self.block3(out)\n","    out = self.block4(out)\n","    out = self.block5(out)\n","    out = self.block6(out)\n","    out = self.avg_pool(out)\n","    out = out.view(out.size(0),-1)\n","    out = F.softmax(self.fc(out, sample))\n","    return out\n","  \n","  def log_prior(self):\n","    return self.conv.log_prior + self.block1.log_prior + self.block2.log_prior + self.block3.log_prior + self.block4.log_prior + self.block5.log_prior + self.block6.log_prior + self.fc.log_prior\n","  \n","  def log_variational_posterior(self):\n","    return self.conv.log_variational_posterior + self.block1.log_variational_posterior + self.block2.log_variational_posterior + self.block3.log_variational_posterior + self.block4.log_variational_posterior + self.block5.log_variational_posterior + self.block6.log_variational_posterior + self.fc.log_variational_posterior\n","  \n","  \n","  def free_energy(self, input, target, batch_size, num_batches, n_samples, T):\n","    outputs = torch.zeros(batch_size, self.num_class).to(DEVICE)\n","    log_prior = torch.zeros(1).to(DEVICE)\n","    log_variational_posterior = torch.zeros(1).to(DEVICE)\n","    negative_log_likelihood = torch.zeros(1).to(DEVICE)\n","    for i in range(n_samples):\n","      output = self(input, sample=True)\n","      outputs +=  output/n_samples\n","      log_prior += self.log_prior()/n_samples\n","      log_variational_posterior += self.log_variational_posterior()/n_samples\n","      negative_log_likelihood += F.nll_loss(torch.log(output+eps), target, size_average=False)/n_samples\n","\n","    # new target function, not absorb T into prior\n","    loss = (log_variational_posterior - log_prior / T) + negative_log_likelihood / T * num_batches \n","\n","    corrects = outputs.argmax(dim=1).eq(target).sum().item()\n","\n","    return loss, log_prior, log_variational_posterior, negative_log_likelihood, corrects\n","\n","  \n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qFyFDrxLgoPg"},"source":["def write_weight_histograms(epoch):\n","  writer.add_histogram('histogram/w1_mu', net.l1.w_mu, epoch)\n","  writer.add_histogram('histogram/w1_rho', net.l1.w_rho, epoch)\n","  writer.add_histogram('histogram/w2_mu', net.l2.w_mu, epoch)\n","  writer.add_histogram('histogram/w2_rho', net.l2.w_rho, epoch)\n","  writer.add_histogram('histogram/w3_mu', net.l3.w_mu, epoch)\n","  writer.add_histogram('histogram/w3_rho', net.l3.w_rho, epoch)\n","\n","def write_loss_scalars(epoch, loss, accuracy, log_prior, log_variational_posterior, negative_log_likelihood):\n","  writer.add_scalar('logs/loss', loss, epoch)\n","  writer.add_scalar('logs/accuracy', accuracy, epoch)\n","  writer.add_scalar('logs/complexity', log_variational_posterior-log_prior, epoch)\n","  writer.add_scalar('logs/negative_log_likelihood', negative_log_likelihood, epoch)\n","\n","\n","def write_test_scalar(epoch, loss, accuracy):\n","  writer.add_scalar('logs/test_loss', loss,epoch)\n","  writer.add_scalar('logs/test_accuracy', accuracy, epoch)\n","\n","def write_sigma(epoch):\n","  writer.add_scalar('sigma/block1', net.block1.sigma_mean,epoch)\n","  writer.add_scalar('sigma/block2', net.block2.sigma_mean,epoch)\n","  writer.add_scalar('sigma/block3', net.block3.sigma_mean,epoch)\n","  writer.add_scalar('sigma/block4', net.block4.sigma_mean,epoch)\n","  writer.add_scalar('sigma/block5', net.block5.sigma_mean,epoch)\n","  writer.add_scalar('sigma/block6', net.block6.sigma_mean,epoch)\n","  writer.add_scalar('sigma/fc', net.fc.sigma_mean,epoch)\n","  \n","  writer.add_scalar('sigmastd/block1', net.block1.sigma_std,epoch)\n","  writer.add_scalar('sigmastd/block2', net.block2.sigma_std,epoch)\n","  writer.add_scalar('sigmastd/block3', net.block3.sigma_std,epoch)\n","  writer.add_scalar('sigmastd/block4', net.block4.sigma_std,epoch)\n","  writer.add_scalar('sigmastd/block5', net.block5.sigma_std,epoch)\n","  writer.add_scalar('sigmastd/block6', net.block6.sigma_std,epoch)\n","  writer.add_scalar('sigmastd/fc', net.fc.sigma_std,epoch)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ldlov8AF_iUb"},"source":["# Train and test\n"]},{"cell_type":"code","metadata":{"id":"548Mk-zGM49W"},"source":["def train(net, optimizer, epoch, trainLoader, batchSize, nSamples ,T):\n","  net.train()\n","  num_batches_train = len(trainLoader)\n","  \n"," # if epoch == 0:\n","  #  write_weight_histograms(epoch)\n","  for batch_idx, (data, target) in enumerate(tqdm(trainLoader)):\n","    data, target = data.to(DEVICE), target.to(DEVICE)\n","    \n","    net.zero_grad()\n","    loss, log_prior, log_variational_posterior, negative_log_likelihood, corrects = net.free_energy(data, target, batchSize, num_batches_train, nSamples,T)\n","    loss.backward()\n","    optimizer.step()\n","\n","    accuracy = corrects / batchSize\n"," # write_loss_scalars(epoch, loss, accuracy, log_prior, log_variational_posterior, negative_log_likelihood)\n"," # write_weight_histograms(epoch)\n"," # write_sigma(epoch)\n","  \n","  return accuracy, loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"htiy9Bp82-t-"},"source":["def test_duringTrain(net, epoch, testLoader, batchSize, nSamples, T):\n","  net.eval()\n","  accuracy = 0\n","  n_corrects = 0\n","  Loss = 0\n","  num_batches_test = len(testLoader)\n","  n_test = batchSize * num_batches_test\n","  outputs = torch.zeros(n_test, 10).to(DEVICE)\n","  correct = torch.zeros(n_test).to(DEVICE)\n","\n","  \n","  with torch.no_grad():\n","    for i, (data, target) in enumerate(testLoader):\n","      data, target = data.to(DEVICE), target.to(DEVICE)\n","      for j in range(nSamples):\n","        output = net(data, sample=True)\n","        outputs[i*batchSize:batchSize*(i+1), :] += output/nSamples\n","        Loss +=  F.nll_loss(torch.log(output), target, size_average=False)/nSamples\n","        # loss is log likelihood\n","        \n","      correct[i*batch_size:batchSize*(i+1)] = (outputs[i*batchSize:batchSize*(i+1), :]).argmax(1).eq(target)\n","        \n","    accuracy = correct.mean()\n","    #write_test_scalar(epoch, Loss, accuracy)\n","    \n","  return accuracy, Loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gec7ra9MD2B-"},"source":["def test(net, testLoader, batchSize, nSamples,T, num_class=10):\n","  # update ECE\n","  net.eval()\n","  accuracy = 0\n","  n_corrects = 0\n","  Loss = 0\n","  num_batches_test = len(testLoader)\n","  n_test = batchSize * num_batches_test\n","  outputs = torch.zeros(n_test, num_class).to(DEVICE)\n","  correct = torch.zeros(n_test).to(DEVICE)\n","  target_all = torch.zeros(n_test).to(DEVICE)\n","  \n","  M = 10\n","  boundary = ((torch.tensor(range(0,M))+1)/10).view(1,-1)\n","  boundary = boundary.repeat(batchSize, 1).to(DEVICE)\n","  \n","  acc_Bm_sum = torch.zeros(M).to(DEVICE)\n","  conf_Bm_sum = torch.zeros(M).to(DEVICE)\n","  Bm = torch.zeros(M).to(DEVICE)\n","  \n","  with torch.no_grad():\n","    for i, (data, target) in enumerate(testLoader):\n","      data, target = data.to(DEVICE), target.to(DEVICE)\n","      target_all[i*batchSize:batchSize*(i+1)] = target\n","      for j in range(nSamples):\n","        output = net(data, sample=True)\n","        outputs[i*batchSize:batchSize*(i+1), :] += output/nSamples\n","        Loss +=  F.nll_loss(torch.log(output), target, size_average=False)/nSamples\n","        # loss is log likelihood\n","        \n","      correct[i*batchSize:batchSize*(i+1)] = (outputs[i*batchSize:batchSize*(i+1), :]).argmax(1).eq(target)\n","      \n","      otemp =outputs[i*batchSize:batchSize*(i+1), :]\n","      p_i,_ = otemp.max(dim=1, keepdims=True)\n","      B = (p_i.le(boundary)*1).argmax(dim=1)\n","          \n","      acc_i = otemp.argmax(1).eq(target)\n","      for m in range(M):\n","        is_m = B.eq(m)\n","        Bm[m] += is_m.sum()\n","        acc_Bm_sum[m] += torch.sum(acc_i * is_m)\n","        conf_Bm_sum[m] += torch.sum(p_i.flatten() * is_m)\n","\n","    accuracy = correct.mean()\n","\n","  ROCAUC = roc_auc_score(target_all.cpu(), outputs.cpu(), multi_class='ovr')\n","  \n","  ECE = (acc_Bm_sum - conf_Bm_sum).abs().sum()/(n_test)\n","\n","  temp = (acc_Bm_sum - conf_Bm_sum)/Bm\n","  temp[temp!=temp]=0\n","  MCE,_ = temp.abs().max(0)\n","\n","  return accuracy, Loss, ECE, MCE, ROCAUC, output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s4Ukeernnoev"},"source":["def test_MoG(net_list, testLoader, batchSize, nSamples,T, num_class=10):\n","  # update ECE\n","  for net in net_list:\n","    net.eval()\n","  accuracy = 0\n","  n_corrects = 0\n","  Loss = 0\n","  num_batches_test = len(testLoader)\n","  n_test = batchSize * num_batches_test\n","  outputs = torch.zeros(n_test, num_class).to(DEVICE)\n","  correct = torch.zeros(n_test).to(DEVICE)\n","  target_all = torch.zeros(n_test).to(DEVICE)\n","  n_list = len(net_list)\n","\n","  M = 10\n","  boundary = ((torch.tensor(range(0,M))+1)/10).view(1,-1)\n","  boundary = boundary.repeat(batchSize, 1).to(DEVICE)\n","  \n","  acc_Bm_sum = torch.zeros(M).to(DEVICE)\n","  conf_Bm_sum = torch.zeros(M).to(DEVICE)\n","  Bm = torch.zeros(M).to(DEVICE)\n","  \n","  with torch.no_grad():\n","    for i, (data, target) in enumerate(testLoader):\n","      data, target = data.to(DEVICE), target.to(DEVICE)\n","      target_all[i*batchSize:batchSize*(i+1)] = target\n","      for k, net in enumerate(net_list):\n","        for j in range(nSamples):\n","          output = net(data, sample=True)\n","          outputs[i*batchSize:batchSize*(i+1), :] += output/(nSamples*n_list)\n","          Loss +=  F.nll_loss(torch.log(output), target, size_average=False)/(nSamples*n_list)\n","          # loss is log likelihood\n","          \n","      correct[i*batchSize:batchSize*(i+1)] = (outputs[i*batchSize:batchSize*(i+1), :]).argmax(1).eq(target)\n","      \n","      otemp = outputs[i*batchSize:batchSize*(i+1), :]\n","      p_i,_ = otemp.max(dim=1, keepdims=True)\n","      B = (p_i.le(boundary)*1).argmax(dim=1)\n","      \n","      acc_i = otemp.argmax(1).eq(target)\n","      for m in range(M):\n","        is_m = B.eq(m)\n","        Bm[m] += is_m.sum()\n","        acc_Bm_sum[m] += torch.sum(acc_i * is_m)\n","        conf_Bm_sum[m] += torch.sum(p_i.flatten() * is_m)\n","\n","    accuracy = correct.mean()\n","\n","  ROCAUC = roc_auc_score(target_all.cpu(), outputs.cpu(), multi_class='ovr')\n","  \n","  ECE = (acc_Bm_sum - conf_Bm_sum).abs().sum()/(n_test)\n","\n","  temp = (acc_Bm_sum - conf_Bm_sum)/Bm\n","  temp[temp!=temp]=0\n","  MCE,_ = temp.abs().max(0)\n","\n","  return accuracy, Loss, ECE, MCE, ROCAUC, output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T1ZF8uDG5zpk"},"source":["def cal_entropy(p):\n","  logP = p.clone()\n","  logP[p==0]=1\n","  logP = torch.log(logP)\n","  return (-logP*p).sum(dim=1)\n","\n","def OOD_test(net, oodLoader, inDis_output, batchSize, nSamples, T, num_class=10):\n","  net.eval()\n","  num_batches_test = len(oodLoader)\n","  n_test = batchSize * num_batches_test\n","  n_inDis = len(inDis_output)\n","\n","  outputs = torch.zeros(n_test, num_class).to(DEVICE)\n","  \n","  target_all = torch.zeros(n_test+n_inDis)\n","  target_all[n_test:] = 1\n","\n","  score1 = torch.zeros(n_test+n_inDis)\n","  score2 = torch.zeros(n_test+n_inDis)\n","\n","  with torch.no_grad():\n","    for i, (data, target) in enumerate(oodLoader):\n","      data = data.to(DEVICE)\n","\n","      for j in range(nSamples):\n","        output = net(data,sample=True)\n","        outputs[i*batch_size:batchSize*(i+1), :] += output/nSamples\n","  entropy = cal_entropy(outputs)\n","  entropy_ave = entropy.mean()\n","  entropy_std = entropy.std()\n","\n","  score1[:n_test],_ = outputs.max(dim=1)\n","  score1[n_test:],_ = inDis_output.max(dim=1)\n","\n","  score2[:n_test] = entropy_ave\n","  score2[n_test:] = cal_entropy(inDis_output).mean()\n","\n","  L2D  = (torch.square(outputs-0.1).sum(dim=1)).mean()\n","  ROCAUC1 = roc_auc_score(target_all, score1, multi_class='ovr', average='weighted')\n","  ROCAUC2 = roc_auc_score(target_all, score2, multi_class='ovr', average='weighted')\n","  return entropy_ave, entropy_std, L2D, ROCAUC1, ROCAUC2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tzVk0d3ZSiER"},"source":["def OOD_test_MoG(net_list, oodLoader, inDis_output, batchSize, nSamples, T, num_class=10):\n","  for net in net_list:\n","    net.eval()\n","  num_batches_test = len(oodLoader)\n","  n_test = batchSize * num_batches_test\n","  n_inDis = len(inDis_output)\n","  n_list = len(net_list)\n","\n","  outputs = torch.zeros(n_test, num_class).to(DEVICE)\n","  \n","  target_all = torch.zeros(n_test+n_inDis)\n","  target_all[n_test:] = 1\n","\n","  score1 = torch.zeros(n_test+n_inDis)\n","  score2 = torch.zeros(n_test+n_inDis)\n","\n","  with torch.no_grad():\n","    for i, (data, target) in enumerate(oodLoader):\n","      data = data.to(DEVICE)\n","      \n","      for k, net in enumerate(net_list):\n","          for j in range(nSamples):\n","            output = net(data,sample=True)\n","            outputs[i*batch_size:batchSize*(i+1), :] += output/(nSamples*n_list)\n","  entropy = cal_entropy(outputs)\n","  entropy_ave = entropy.mean()\n","  entropy_std = entropy.std()\n","\n","  score1[:n_test],_ = outputs.max(dim=1)\n","  score1[n_test:],_ = inDis_output.max(dim=1)\n","\n","  score2[:n_test] = entropy_ave\n","  score2[n_test:] = cal_entropy(inDis_output).mean()\n","\n","  L2D  = (torch.square(outputs-0.1).sum(dim=1)).mean()\n","  ROCAUC1 = roc_auc_score(target_all, score1, multi_class='ovr', average='weighted')\n","  ROCAUC2 = roc_auc_score(target_all, score2, multi_class='ovr', average='weighted')\n","  return entropy_ave, entropy_std, L2D, ROCAUC1, ROCAUC2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3t4TVO14kLqr"},"source":["def update_lr(optimizer,lr):\n","  for param_group in optimizer.param_groups:\n","    param_group['lr']= lr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jJ9yvXzHkNiX"},"source":["def reset_net(net, pretrained_net):\n","  net.conv.w_mu.data.copy_(pretrained_net.conv.w_mu.data)\n","  net.block1.conv1.w_mu.data.copy_(pretrained_net.block1.conv1.w_mu.data)\n","  net.block1.conv2.w_mu.data.copy_(pretrained_net.block1.conv2.w_mu.data)\n","  net.block2.conv1.w_mu.data.copy_(pretrained_net.block2.conv1.w_mu.data)\n","  net.block2.conv2.w_mu.data.copy_(pretrained_net.block2.conv2.w_mu.data)\n","  net.block3.conv1.w_mu.data.copy_(pretrained_net.block3.conv1.w_mu.data)\n","  net.block3.conv2.w_mu.data.copy_(pretrained_net.block3.conv2.w_mu.data)\n","  net.block4.conv1.w_mu.data.copy_(pretrained_net.block4.conv1.w_mu.data)\n","  net.block4.conv2.w_mu.data.copy_(pretrained_net.block4.conv2.w_mu.data)\n","  net.block5.conv1.w_mu.data.copy_(pretrained_net.block5.conv1.w_mu.data)\n","  net.block5.conv2.w_mu.data.copy_(pretrained_net.block5.conv2.w_mu.data)\n","  net.block6.conv1.w_mu.data.copy_(pretrained_net.block6.conv1.w_mu.data)\n","  net.block6.conv2.w_mu.data.copy_(pretrained_net.block6.conv2.w_mu.data)\n","  net.fc.w_mu.data.copy_(pretrained_net.fc.w_mu.data)\n","  net.fc.b_mu.data.copy_(pretrained_net.fc.b_mu.data)\n","  return net"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wYeUu1l_51ja"},"source":["# PreTrained Model"]},{"cell_type":"code","metadata":{"id":"noHMHrsQ5fv_"},"source":["# nonBayesian Network\n","class myLinear(nn.Module):\n","  def __init__(self, n_input, n_output, sigma1):\n","    super().__init__()\n","    self.n_input = n_input\n","    self.n_output = n_output\n","    #self.T = T\n","    #self.sigma1 = sigma1\n","\n","    self.w_mu = nn.Parameter(torch.Tensor(n_output,n_input).normal_(0,math.sqrt(2/n_input))) #todo\n","    \n","\n","    self.b_mu = nn.Parameter(torch.Tensor(n_output).normal_(0,math.sqrt(2/n_input)))\n","    \n","  def forward(self, input, sample=False):\n","    \n","    w = self.w_mu\n","    b = self.b_mu\n","    \n","  \n","    return F.linear(input, w, b)\n","\n","class myConv2D(nn.Module):\n","  def __init__(self, in_channels, out_channels, sigma1, kernel_size=3, stride=1, padding=1):\n","    super().__init__()\n","    self.in_channels = in_channels\n","    self.out_channels = out_channels\n","    self.kernel_size = kernel_size\n","    self.stride = stride\n","    self.padding = padding\n","\n","    self.w_mu = nn.Parameter(torch.Tensor(out_channels,in_channels, kernel_size, kernel_size))\n","    self.reset_para()\n","  \n","  def reset_para(self):\n","    nn.init.kaiming_uniform_(self.w_mu, a=math.sqrt(5))\n","  \n","  def forward(self, input, sample=False):\n","    \n","    w = self.w_mu\n","    \n","    \n","    return F.conv2d(input, w, bias=None, stride=self.stride, padding=self.padding)\n","\n","\n","def myConv3x3(in_channels, out_channels, sigma1, stride=1):\n","  return myConv2D(in_channels, out_channels, sigma1, kernel_size=3,stride=stride, padding=1)\n","\n","\n","class myResidualBlock(nn.Module):\n","  def __init__(self, in_channels, out_channels, sigma1, stride=1, downsample=None):\n","    super().__init__()\n","    self.conv1 = myConv3x3(in_channels, out_channels, sigma1, stride)\n","    self.frn1 = nn.BatchNorm2d(out_channels)\n","    self.tlu1 = nn.ReLU(inplace=True)\n","    self.conv2 = myConv3x3(out_channels, out_channels, sigma1)\n","    self.frn2 = nn.BatchNorm2d(out_channels)\n","    self.tlu2 = nn.ReLU(inplace=True)\n","    self.downsample = downsample\n","  \n","  def forward(self, x):\n","    residual = x\n","    out = self.conv1(x)\n","    out = self.frn1(out)\n","    out = self.tlu1(out)\n","    out = self.conv2(out)\n","    out = self.frn2(out)\n","    if self.downsample:\n","      residual = self.downsample(x)\n","    out += residual\n","    out = self.tlu2(out)\n","    return out\n","  \n","class myResNet14(nn.Module):\n","  def __init__(self, sigma1, num_class=10):\n","    super().__init__()\n","    self.in_channels = 16\n","    self.conv = myConv3x3(3,16, sigma1)\n","    self.frn = nn.BatchNorm2d(16)\n","    self.tlu = nn.ReLU(inplace=True)\n","\n","    self.block1 = myResidualBlock(16,16,sigma1)\n","    self.block2 = myResidualBlock(16,16,sigma1)\n","\n","    downsample1 = nn.Sequential(myConv3x3(16,32,sigma1,2), nn.BatchNorm2d(32))\n","    self.block3 = myResidualBlock(16,32,sigma1,2,downsample1)\n","    self.block4 = myResidualBlock(32,32,sigma1)\n","\n","    downsample2 = nn.Sequential(myConv3x3(32,64,sigma1,2), nn.BatchNorm2d(64))\n","    self.block5 = myResidualBlock(32,64,sigma1,2,downsample2)\n","    self.block6 = myResidualBlock(64,64,sigma1)\n","\n","    self.avg_pool = nn.AvgPool2d(8)\n","    self.fc = myLinear(64, num_class, sigma1)\n","\n","  def forward(self, x, sample=False):\n","    out = self.conv(x)\n","    out = self.frn(out)\n","    out = self.tlu(out)\n","    out = self.block1(out)\n","    out = self.block2(out)\n","    out = self.block3(out)\n","    out = self.block4(out)\n","    out = self.block5(out)\n","    out = self.block6(out)\n","    out = self.avg_pool(out)\n","    out = out.view(out.size(0),-1)\n","    out = F.softmax(self.fc(out, sample))\n","    return out\n","  \n","   \n","  \n","  def free_energy(self, input, target, batch_size, num_batches, n_samples, T):\n","    negative_log_likelihood = torch.zeros(1).to(DEVICE)\n","    for i in range(n_samples):\n","      output = self(input, sample=True)\n","      negative_log_likelihood += F.nll_loss(torch.log(output+eps), target, size_average=False)/n_samples\n","\n","    # new target function, not absorb T into prior\n","    loss = negative_log_likelihood / T * num_batches \n","\n","    corrects = output.argmax(dim=1).eq(target).sum().item()\n","\n","    return loss, corrects,0,0,0\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PBwKqkX85kek"},"source":["pretrained_net = myResNet14(1,num_class=100).to(DEVICE)\n","with open(os.path.join(path, \"pretrained/net2.pkl\"), \"rb\") as f:\n","    pretrained_net.load_state_dict (torch.load(f))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uj7ZwtCS5xL3","executionInfo":{"elapsed":3003,"status":"ok","timestamp":1627898974805,"user":{"displayName":"you zhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh32ZF0Azcg0NujEfCqvGOZivVXrQHo4lrJnh5x=s64","userId":"05412124325255519397"},"user_tz":-60},"outputId":"a3937363-b41d-4e0f-8a76-e77061e11d7a"},"source":["testAcc, testLoss, testECE1, testMCE1, AUCROC1, output = test(pretrained_net, test_loader, 128, 10, 1, num_class=100)"],"execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_29485/47708021.py:108: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  out = F.softmax(self.fc(out, sample))\n","/opt/conda/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"47gDZ9IL5xzT","executionInfo":{"elapsed":4,"status":"ok","timestamp":1627898974806,"user":{"displayName":"you zhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh32ZF0Azcg0NujEfCqvGOZivVXrQHo4lrJnh5x=s64","userId":"05412124325255519397"},"user_tz":-60},"outputId":"314952fd-6f25-42ff-f486-376579472532"},"source":["testAcc"],"execution_count":null,"outputs":[{"data":{"text/plain":["tensor(0.4602, device='cuda:0')"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"uT39S5ZnSiES"},"source":["0.4730; 0.4696; 0.4601"]},{"cell_type":"markdown","metadata":{"id":"tzmhlcP_oypG"},"source":["# initialise with 3 SGD solutions"]},{"cell_type":"code","metadata":{"id":"ifrcDtP4o4T0"},"source":["batch_size = 128\n","n_samples = 1\n","T_list = torch.pow(10,-1*torch.tensor(range(0,35,5))/10).to(DEVICE)\n","sigma = torch.sqrt(torch.tensor(1))\n","epochs = 1\n","max_lr = 0.0001\n","curr_lr = 0.0001\n","MoG_net = []"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F8QBX94gqZ0O"},"source":["testAcc for pretrained net\n","0.8263\n","0.8321\n","0.8229"]},{"cell_type":"code","metadata":{"id":"BSZJ9K26VDO2"},"source":["for t,T in enumerate(T_list):   \n","    for i in range(3):\n","      print(i)\n","\n","      pretrained_net = myResNet14(1,num_class=100).to(DEVICE)\n","      with open(os.path.join(path,f\"pretrained/net{i}.pkl\"), \"rb\") as f:\n","          pretrained_net.load_state_dict (torch.load(f))\n","\n","      net = BayesianResNet14(ResidualBlock, sigma, num_class=100).to(DEVICE)\n","      net = reset_net(net, pretrained_net)\n","      optimizer = optim.Adam(net.parameters(),lr=curr_lr)\n","\n","      for epoch in range(epochs):\n","        trainAcc, trainLoss = train(net, optimizer, epoch+i*epochs, training_loader, batch_size, n_samples,T)\n","        curr_lr = max_lr/2 * (1+math.cos((epoch)/epochs*math.pi))\n","        update_lr(optimizer,curr_lr)\n","      with open(os.path.join(path,f\"pretrainedCosine/net{t}{i}.pt\"), \"wb\") as f:\n","        torch.save(net.state_dict(),f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uRZQg2CZSiES"},"source":["testAcc_MoG = torch.zeros(7,).to(DEVICE)\n","testLoss_MoG = torch.zeros(7,).to(DEVICE)\n","testECE_MoG = torch.zeros(7,).to(DEVICE)\n","testROCAUC_MoG = torch.zeros(7,).to(DEVICE)\n","entropy_ave_MoG = torch.zeros(7,).to(DEVICE)\n","entropy_std_MoG = torch.zeros(7,).to(DEVICE)\n","L2D_MoG = torch.zeros(7,).to(DEVICE)\n","ROCAUC1_MoG = torch.zeros(7,).to(DEVICE)\n","ROCAUC2_MoG = torch.zeros(7,).to(DEVICE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wc6vmUgJSiES","outputId":"320014f7-830b-4a4b-8576-d732dc41c694"},"source":["for t,T in enumerate(T_list):\n","    print(t)\n","    MoG_net = []\n","    for i in range(3):\n","        net = BayesianResNet14(ResidualBlock, sigma, num_class=100).to(DEVICE)\n","        with open(os.path.join(path,f\"pretrainedCosine/net{t}{i}.pt\"), \"rb\") as f:\n","            net.load_state_dict(torch.load(f))\n","        MoG_net.append(net)\n","    testAcc_MoG[t], testLoss_MoG[t], testECE_MoG[t], _, testROCAUC_MoG[t], out =test_MoG(MoG_net, test_loader, batch_size, 17,T, num_class=100)\n","    entropy_ave_MoG[t], entropy_std_MoG[t], L2D_MoG[t], ROCAUC1_MoG[t], ROCAUC2_MoG[t] = OOD_test_MoG(MoG_net, svhn_loader, out, batch_size, 17, T, num_class=100)\n","    with open(os.path.join(path, \"results/test_accuracy.pt\"), \"wb\") as f:\n","      torch.save(testAcc_MoG.cpu(),f)\n","\n","    with open(os.path.join(path,\"results/test_loss.pt\"), \"wb\") as f:\n","      torch.save(testLoss_MoG.cpu(),f)\n","\n","    with open(os.path.join(path,\"results/testECE.pt\"), \"wb\") as f:\n","      torch.save(testECE_MoG.cpu(),f)\n","\n","    with open(os.path.join(path,\"results/entropy_ave.pt\"), \"wb\") as f:\n","      torch.save(entropy_ave_MoG.cpu(),f)\n","\n","    with open(os.path.join(path,\"results/entropy_std.pt\"), \"wb\") as f:\n","      torch.save(entropy_std_MoG.cpu(),f)\n","\n","    with open(os.path.join(path,\"results/L2D.pt\"), \"wb\") as f:\n","      torch.save(L2D_MoG.cpu(),f)\n","\n","    with open(os.path.join(path,\"results/test_ROCAUC.pt\"), \"wb\") as f:\n","      torch.save(testROCAUC_MoG.cpu(),f)\n","\n","    with open(os.path.join(path,\"results/ood_ROCAUC1.pt\"), \"wb\") as f:\n","      torch.save(ROCAUC1_MoG.cpu(),f)\n","\n","    with open(os.path.join(path,\"results/ood_ROCAUC2.pt\"), \"wb\") as f:\n","      torch.save(ROCAUC2_MoG.cpu(),f)      \n"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["0\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_29485/1559113692.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  out = F.softmax(self.fc(out, sample))\n","/opt/conda/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n","/tmp/ipykernel_29485/1559113692.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  out = F.softmax(self.fc(out, sample))\n"]},{"name":"stdout","output_type":"stream","text":["1\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_29485/1559113692.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  out = F.softmax(self.fc(out, sample))\n","/opt/conda/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n"]}]},{"cell_type":"code","metadata":{"id":"AnMZ-h_ESiET","outputId":"b7a1933b-0128-481b-8f54-b4e8912dd90e"},"source":["ROCAUC1_MoG"],"execution_count":null,"outputs":[{"data":{"text/plain":["tensor([0.7824, 0.7561, 0.8156, 0.7863, 0.7699, 0.8001, 0.8124],\n","       device='cuda:0')"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"bkNhbeR7SiET","outputId":"b8fe23de-bdb0-42e9-caf5-31b7e313ea76"},"source":["testECE_MoG"],"execution_count":null,"outputs":[{"data":{"text/plain":["tensor([0.0573, 0.0513, 0.0486, 0.0498, 0.0493, 0.0532, 0.0488],\n","       device='cuda:0')"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"gQUs6YsaSiET","outputId":"e3c99878-f627-4bf2-a087-f6e1b7eaae1f"},"source":["testAcc_MoG"],"execution_count":null,"outputs":[{"data":{"text/plain":["tensor([0.6021, 0.6019, 0.6039, 0.6019, 0.5990, 0.6024, 0.5991],\n","       device='cuda:0')"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"13KJfJqJSiET","outputId":"1321e399-ac62-4cbe-be4f-05c50acadf09"},"source":["entropy_ave_MoG"],"execution_count":null,"outputs":[{"data":{"text/plain":["tensor([2.4446, 2.3381, 2.5002, 2.4460, 2.4513, 2.4970, 2.4664],\n","       device='cuda:0')"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}]}]}