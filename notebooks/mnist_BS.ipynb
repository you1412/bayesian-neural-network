{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"mnist_BS.ipynb","provenance":[{"file_id":"1O-qLjtaRYZUaXdi_CaP53NM_4sqXyYpG","timestamp":1624889680305},{"file_id":"1jf7x0sFhk4SJJo_Y6EeiKk1R3BZUEfMH","timestamp":1624439897971},{"file_id":"1jp1ZUm4traW4J54phPKkqAS9-xxMVUVq","timestamp":1624287705254},{"file_id":"19SEmN5wXK1329I6yCiebwx7k_F63_n5x","timestamp":1623965488620}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"cells":[{"cell_type":"code","metadata":{"id":"7kbAV38tE8Ni","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629925446889,"user_tz":-60,"elapsed":242,"user":{"displayName":"you zhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh32ZF0Azcg0NujEfCqvGOZivVXrQHo4lrJnh5x=s64","userId":"05412124325255519397"}},"outputId":"9bbc3b7a-394d-445f-ff6c-3fc860453183"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"20Z27Nippj46"},"source":["path = \"/content/drive/MyDrive/dissertationCode\"\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j5Wedenueous"},"source":["import math\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from tqdm import tqdm, trange\n","from sklearn.metrics import roc_auc_score\n","import random\n","\n","import pickle as pkl\n","\n","#from torch.utils.tensorboard import SummaryWriter\n","#writer = SummaryWriter(log_dir=\"/content/drive/MyDrive/Colab Notebooks/tensorboard\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gobe1On_fYzm","executionInfo":{"status":"ok","timestamp":1629929089975,"user_tz":-60,"elapsed":5,"user":{"displayName":"you zhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh32ZF0Azcg0NujEfCqvGOZivVXrQHo4lrJnh5x=s64","userId":"05412124325255519397"}},"outputId":"d0a3689d-af09-42ac-ba5c-74918023f3d7"},"source":["DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","LOADER_KWARGS = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else{}\n","print(torch.cuda.is_available()) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["True\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BvDEdpeS_WRI","executionInfo":{"status":"ok","timestamp":1629929091383,"user_tz":-60,"elapsed":403,"user":{"displayName":"you zhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh32ZF0Azcg0NujEfCqvGOZivVXrQHo4lrJnh5x=s64","userId":"05412124325255519397"}},"outputId":"171346bc-9b2d-482b-d0e1-ffdba94c1ca9"},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Wed Aug 25 22:04:50 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.57.02    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   43C    P0    28W / 250W |      2MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Z2anPvLv-b9X"},"source":["# Download data"]},{"cell_type":"code","metadata":{"id":"1nQTZKvngo5Y","jupyter":{"outputs_hidden":true},"tags":[]},"source":["training_data = datasets.MNIST(root='..data', train=True, download=True, transform=transforms.ToTensor())\n","test_data = datasets.MNIST(root='..data', train=False, download=True, transform=transforms.ToTensor())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IaaPtLXFh5fB"},"source":["train_set, val_set = torch.utils.data.random_split(training_data,[50000,10000])\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True, drop_last=True, **LOADER_KWARGS)\n","val_loader = torch.utils.data.DataLoader(val_set, batch_size=128, shuffle=True, drop_last=True, ** LOADER_KWARGS)\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=128, shuffle=True, drop_last=True, **LOADER_KWARGS)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xaaACd17gSGD"},"source":["training_loader = torch.utils.data.DataLoader(training_data, batch_size=128, shuffle=True, drop_last=True, **LOADER_KWARGS)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oT6KE449cBU5"},"source":["with open(os.path.join(path, \"training_loader.pt\"), \"wb\") as f:\n","  torch.save(training_loader,f)\n","\n","with open(os.path.join(path, \"test_loader.pt\"), \"wb\") as f:\n","  torch.save(test_loader,f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JrGHYPeZp-Lp"},"source":["# out-of-distribution dataset: notMNIST"]},{"cell_type":"code","metadata":{"id":"yOJ8IyRtp9-S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629826791973,"user_tz":-60,"elapsed":172084,"user":{"displayName":"you zhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh32ZF0Azcg0NujEfCqvGOZivVXrQHo4lrJnh5x=s64","userId":"05412124325255519397"}},"outputId":"fe297667-83b6-4776-c176-9f8a1d388e5a"},"source":["%%time\n","\n","import shutil\n","shutil.rmtree(\"/content/drive/MyDrive/dissertationCode/notMNIST_small\")\n","\n","f1 = \"/content/drive/MyDrive/dissertationCode/notMNIST_small.tar.gz\"\n","f1_extract = \"/content/drive/MyDrive/dissertationCode/notMNIST_small/\"\n","shutil.unpack_archive(f1, f1_extract)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 8.51 s, sys: 8.33 s, total: 16.8 s\n","Wall time: 2min 51s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vstnOwN2p96_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629826817926,"user_tz":-60,"elapsed":504,"user":{"displayName":"you zhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh32ZF0Azcg0NujEfCqvGOZivVXrQHo4lrJnh5x=s64","userId":"05412124325255519397"}},"outputId":"5c5b89bf-4226-4e48-f301-6494033d7254"},"source":["ls /content/drive/MyDrive/dissertationCode/notMNIST_small/notMNIST_small/A/ | head -10"],"execution_count":null,"outputs":[{"output_type":"stream","text":["MDEtMDEtMDAudHRm.png\n","MDRiXzA4LnR0Zg==.png\n","MjAwcHJvb2Ztb29uc2hpbmUgcmVtaXgudHRm.png\n","MlJlYmVsc0RldXgtQmxhY2sub3Rm.png\n","MlRvb24gU2hhZG93LnR0Zg==.png\n","MlRvb24yIFNoYWRvdy50dGY=.png\n","MTAuMTUgU2F0dXJkYXkgTmlnaHQgQlJLLnR0Zg==.png\n","MTFTMDEgQmxhY2sgVHVlc2RheSBPZmZzZXQudHRm.png\n","MTggSG9sZXMgQlJLLnR0Zg==.png\n","MTh0aENlbnR1cnkudHRm.png\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Uos3v6fkp94S"},"source":["from PIL import Image\n","import os\n","\n","path = \"/content/drive/MyDrive/dissertationCode/notMNIST_small/notMNIST_small/\"\n","\n","fnames = []\n","for root, dir, files in os.walk(path):\n","  for f in files:\n","    if f.endswith('.png'):\n","      fnames.append(os.path.join(root,f))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UaDsqTeyp91g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629826821343,"user_tz":-60,"elapsed":164,"user":{"displayName":"you zhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh32ZF0Azcg0NujEfCqvGOZivVXrQHo4lrJnh5x=s64","userId":"05412124325255519397"}},"outputId":"2fbf1ccb-1c19-4999-e4db-cf43a4e78fe8"},"source":["print(len(fnames))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["18726\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BNB1p-Vnp9yt"},"source":["# filter out bad images\n","def filter_out_bad_images(fnames):\n","  from matplotlib.pyplot import imread\n","  from joblib import Parallel, delayed\n","  import multiprocessing\n","\n","  def mark_bad_image(f):\n","    try:\n","      im = imread(str(f))\n","    except Exception as e:\n","      print('could not read:', f, ':', e, '-it\\'s ok, skipping')\n","      return f\n","  \n","  # proper filtering\n","  bad_images = Parallel(n_jobs=4)(delayed(mark_bad_image)(f) for f in tqdm(fnames))\n","  bad_images = [f for f in bad_images if f!=None]\n","  print(bad_images)\n","  return [f for f in fnames if f not in bad_images]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4udENk77p9vz"},"source":["label_folder = ['A','B','C','D','E','F','G','H','I','J']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F-gR0-IvqONg"},"source":["def diff_fname(path):\n","  fname = []\n","  for root, dir, files in os.walk(path):\n","    for f in files:\n","      if f.endswith('.png'):\n","        fname.append(os.path.join(root,f))\n","  fname = filter_out_bad_images(fname)\n","  return fname\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lKJuyZs3qOKh"},"source":["# sort root of different picture according to their labels\n","fnames = []\n","main_path=\"/content/drive/MyDrive/dissertationCode/notMNIST_small/notMNIST_small/\"\n","for char in label_folder:\n","  path = os.path.join(main_path,char)\n","  fname = diff_fname(path)\n","  fnames.append(fname)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LHy9dnRuqOIC"},"source":["import pandas as pd\n","from torchvision.io import read_image\n","from torchvision import transforms\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LatFM7h_qOFn"},"source":["class notMNIST(Dataset):\n","  def __init__(self, img_label, img_dir, transform=None):\n","    self.img_labels = img_label\n","    self.img_dir = img_dir\n","    self.transform = transform\n","\n","  def __len__(self):\n","    return len(self.img_labels)\n","  \n","  def __getitem__(self,idx):\n","    img_path = self.img_dir[idx]\n","    image = read_image(img_path)/255.\n","    label = torch.tensor(self.img_labels[idx], dtype=torch.long)\n","    #if self.transform:\n","      #image = self.transform(image)\n","    \n","    return image, label"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HfVX1GrsqVN4"},"source":["whole_fnames = fnames[0]\n","whole_label = np.zeros(len(fnames[0]))\n","for i in range(1,10):\n","  print(len(whole_fnames))\n","  whole_fnames = whole_fnames + fnames[i]\n","  Label = np.ones(len(fnames[i]))*i\n","  whole_label = np.concatenate((whole_label, Label), axis=0)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YbSYcrBvqOC5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629826884840,"user_tz":-60,"elapsed":189,"user":{"displayName":"you zhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh32ZF0Azcg0NujEfCqvGOZivVXrQHo4lrJnh5x=s64","userId":"05412124325255519397"}},"outputId":"85dbbf7d-e316-4889-a077-7bda5aac73f9"},"source":["notMNIST_dataset = notMNIST(whole_label, whole_fnames)\n","notMNIST_dataloader = DataLoader(notMNIST_dataset, batch_size=128, shuffle=True, drop_last=True)\n","print(notMNIST_dataloader)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<torch.utils.data.dataloader.DataLoader object at 0x7fa91b83f310>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AWFj4-ZfqcDC"},"source":["# reset path"]},{"cell_type":"code","metadata":{"id":"yCWcUojsp9qW"},"source":["path = \"/content/drive/MyDrive/dissertationCode\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e_m9LI9c_cC4"},"source":["# Network"]},{"cell_type":"code","metadata":{"id":"4X-PCes7JhX4"},"source":["eps = 1e-10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yUnw6UmHjKTt"},"source":["class Gaussian:\n","  def __init__(self, mu, rho):\n","    self.mu = mu\n","    self.rho = rho\n","    self.normal = torch.distributions.Normal(0,1)\n","  \n","  @property\n","  def sigma(self):\n","    return torch.log1p(torch.exp(self.rho))\n","  \n","  def sample(self):\n","    epsilon = self.normal.sample(self.rho.size()).to(DEVICE)\n","    return self.mu + self.sigma * epsilon\n","  \n","  def log_prob(self, input):\n","    return (-math.log(math.sqrt(2 * math.pi)) - torch.log(self.sigma+eps) - ((input - self.mu) ** 2) / (2 * self.sigma ** 2)).sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3uhSN4csp8wj"},"source":["class GaussianPrior:\n","  def __init__(self,mu,sigma):\n","    self.mu = mu\n","    self.sigma = sigma\n","  \n","  def log_prob(self,input):\n","    return (-math.log(math.sqrt(2 * math.pi)) - torch.log(self.sigma) - ((input - self.mu) ** 2) / (2 * self.sigma ** 2)).sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R1xsPOzFlFy2"},"source":["class BayesianLinear(nn.Module):\n","  def __init__(self, n_input, n_output, sigma1,T):\n","    super().__init__()\n","    self.n_input = n_input\n","    self.n_output = n_output\n","\n","    self.w_mu = nn.Parameter(torch.Tensor(3,n_output,n_input).normal_(0,math.sqrt(2/n_input)))\n","    self.w_rho = nn.Parameter(torch.Tensor(3,n_output, n_input).uniform_(-5,-4))\n","    self.w = Gaussian(self.w_mu, self.w_rho)\n","\n","    self.b_mu = nn.Parameter(torch.Tensor(3,n_output).normal_(0,math.sqrt(2/n_input)))\n","    self.b_rho = nn.Parameter(torch.Tensor(3,n_output).uniform_(-5,-4))\n","    self.b = Gaussian(self.b_mu, self.b_rho)\n","\n","\n","    #Prior: Gaussian\n","    self.w_prior = GaussianPrior(0,sigma1)\n","    self.b_prior = GaussianPrior(0,sigma1)\n","    \n","    self.log_prior = 0 \n","    self.log_variational_posterior= 0\n","    #self.KL = 0\n","    self.sigma_mean = 0\n","    self.sigma_std = 0\n","  \n","  def forward(self, input, sample=False):\n","    if self.training or sample:\n","      w = self.w.sample()\n","      b = self.b.sample()\n","      cc = random.randint(0,2)\n","      w = w[cc,:,:]\n","      b = b[cc,:]\n","      w_mat = w.repeat(3,1,1).to(DEVICE)\n","      b_mat = b.repeat(3,1,1).to(DEVICE)\n","    else:\n","      w = self.w_mu\n","      b = self.b_mu\n","      w_mat = w\n","      b_mat = b\n","    \n","    self.log_prior = self.w_prior.log_prob(w_mat)/3 + self.b_prior.log_prob(b_mat)/3\n","    self.log_variational_posterior = self.w.log_prob(w_mat)/3 + self.b.log_prob(b_mat)/3\n","    \n","    self.sigma_mean = self.w.sigma.mean()\n","    self.sigma_std = self.w.sigma.std()\n","    \n","    \n","    return F.linear(input, w, b)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bbzny6bHt39o"},"source":["class BayesianNetwork(nn.Module):\n","  def __init__(self, n_units, sigma1, T):\n","    super().__init__()\n","    self.l1 = BayesianLinear(28*28, n_units, sigma1, T)\n","    self.l2 = BayesianLinear(n_units, n_units, sigma1, T)\n","    self.l3 = BayesianLinear(n_units, 10, sigma1, T)\n","\n","  def forward(self, x, sample=False):\n","    x = x.view(-1,28*28)\n","    x = F.relu(self.l1(x, sample),inplace=False)\n","    x = F.relu(self.l2(x, sample), inplace=False)\n","    x = F.softmax(self.l3(x, sample))\n","    return x\n","  \n","  def log_prior(self):\n","    return self.l1.log_prior + self.l2.log_prior + self.l3.log_prior\n","  \n","  def log_variational_posterior(self):\n","    return self.l1.log_variational_posterior + self.l2.log_variational_posterior + self.l3.log_variational_posterior\n","  \n","  def KL_q_p(self):\n","    return self.l1.KL + self.l2.KL + self.l3.KL\n","\n","  def free_energy(self, input, target, batch_size, num_batches, n_samples, T):\n","    outputs = torch.zeros(batch_size, 10).to(DEVICE)\n","    log_prior = torch.zeros(1).to(DEVICE)\n","    log_variational_posterior = torch.zeros(1).to(DEVICE)\n","    negative_log_likelihood = torch.zeros(1).to(DEVICE)\n","    for i in range(n_samples):\n","      output = self(input, sample=True)\n","      outputs +=  output/n_samples\n","      log_prior += self.log_prior()/n_samples\n","      log_variational_posterior += self.log_variational_posterior()/n_samples\n","      negative_log_likelihood += F.nll_loss(torch.log(output+eps), target, size_average=False)/n_samples\n","\n","    # new target function, not absorb T into prior\n","    loss = (log_variational_posterior - log_prior / T) + negative_log_likelihood / T * num_batches \n","\n","    corrects = outputs.argmax(dim=1).eq(target).sum().item()\n","\n","    return loss, log_prior, log_variational_posterior, negative_log_likelihood, corrects\n","\n","  \n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qFyFDrxLgoPg"},"source":["def write_weight_histograms(epoch):\n","  writer.add_histogram('histogram/w1_mu', net.l1.w_mu, epoch)\n","  writer.add_histogram('histogram/w1_rho', net.l1.w_rho, epoch)\n","  writer.add_histogram('histogram/w2_mu', net.l2.w_mu, epoch)\n","  writer.add_histogram('histogram/w2_rho', net.l2.w_rho, epoch)\n","  writer.add_histogram('histogram/w3_mu', net.l3.w_mu, epoch)\n","  writer.add_histogram('histogram/w3_rho', net.l3.w_rho, epoch)\n","\n","def write_loss_scalars(epoch, loss, accuracy, log_prior, log_variational_posterior, negative_log_likelihood):\n","  writer.add_scalar('logs/loss', loss, epoch)\n","  writer.add_scalar('logs/accuracy', accuracy, epoch)\n","  writer.add_scalar('logs/complexity', log_variational_posterior-log_prior, epoch)\n","  writer.add_scalar('logs/negative_log_likelihood', negative_log_likelihood, epoch)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ldlov8AF_iUb"},"source":["# Train and test\n"]},{"cell_type":"code","metadata":{"id":"548Mk-zGM49W"},"source":["def train(net, optimizer, epoch, trainLoader, batchSize, nSamples ,T):\n","  net.train()\n","  num_batches_train = len(trainLoader)\n","  \n","  #if epoch == 0:\n","   # write_weight_histograms(epoch)\n","  for batch_idx, (data, target) in enumerate(tqdm(trainLoader)):\n","    data, target = data.to(DEVICE), target.to(DEVICE)\n","    \n","    net.zero_grad()\n","    loss, log_prior, log_variational_posterior, negative_log_likelihood, corrects = net.free_energy(data, target, batchSize, num_batches_train, nSamples,T)\n","    loss.backward()\n","    optimizer.step()\n","\n","    accuracy = corrects / batchSize\n","  #write_loss_scalars(epoch, loss, accuracy, log_prior, log_variational_posterior, negative_log_likelihood)\n"," # write_weight_histograms(epoch)\n","\n","  return accuracy, loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H_GWoaFwWmJl"},"source":["def test_duringTrain(net, epoch, testLoader, batchSize, nSamples, T):\n","  net.eval()\n","  accuracy = 0\n","  n_corrects = 0\n","  Loss = 0\n","  num_batches_test = len(testLoader)\n","  n_test = batchSize * num_batches_test\n","  outputs = torch.zeros(n_test, 10).to(DEVICE)\n","  correct = torch.zeros(n_test).to(DEVICE)\n","\n","  \n","  with torch.no_grad():\n","    for i, (data, target) in enumerate(testLoader):\n","      data, target = data.to(DEVICE), target.to(DEVICE)\n","      for j in range(nSamples):\n","        output = net(data, sample=True)\n","        outputs[i*batchSize:batchSize*(i+1), :] += output/nSamples\n","        Loss +=  F.nll_loss(torch.log(output), target, size_average=False)/nSamples\n","        # loss is log likelihood\n","        \n","      correct[i*batch_size:batchSize*(i+1)] = (outputs[i*batchSize:batchSize*(i+1), :]).argmax(1).eq(target)\n","        \n","    accuracy = correct.mean()\n","    #write_test_scalar(epoch, Loss, accuracy)\n","    \n","  return accuracy, Loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gec7ra9MD2B-"},"source":["def test(net, testLoader, batchSize, nSamples,T):\n","  # update ECE\n","  net.eval()\n","  accuracy = 0\n","  n_corrects = 0\n","  Loss = 0\n","  num_batches_test = len(testLoader)\n","  n_test = batchSize * num_batches_test\n","  outputs = torch.zeros(n_test, 10).to(DEVICE)\n","  correct = torch.zeros(n_test).to(DEVICE)\n","  target_all = torch.zeros(n_test).to(DEVICE)\n","  \n","  M = 10\n","  boundary = ((torch.tensor(range(0,M))+1)/10).view(1,-1)\n","  boundary = boundary.repeat(batchSize, 1).to(DEVICE)\n","  \n","  acc_Bm_sum = torch.zeros(M).to(DEVICE)\n","  conf_Bm_sum = torch.zeros(M).to(DEVICE)\n","  Bm = torch.zeros(M).to(DEVICE)\n","  \n","  with torch.no_grad():\n","    for i, (data, target) in enumerate(testLoader):\n","      data, target = data.to(DEVICE), target.to(DEVICE)\n","      target_all[i*batchSize:batchSize*(i+1)] = target\n","      for j in range(nSamples):\n","        output = net(data, sample=True)\n","        outputs[i*batchSize:batchSize*(i+1), :] += output/nSamples\n","        Loss +=  F.nll_loss(torch.log(output), target, size_average=False)/nSamples\n","        # loss is log likelihood\n","        \n","      correct[i*batchSize:batchSize*(i+1)] = (outputs[i*batchSize:batchSize*(i+1), :]).argmax(1).eq(target)\n","      \n","      otemp =outputs[i*batchSize:batchSize*(i+1), :]\n","      p_i,_ = otemp.max(dim=1, keepdims=True)\n","      B = (p_i.le(boundary)*1).argmax(dim=1)\n","          \n","      acc_i = otemp.argmax(1).eq(target)\n","      for m in range(M):\n","        is_m = B.eq(m)\n","        Bm[m] += is_m.sum()\n","        acc_Bm_sum[m] += torch.sum(acc_i * is_m)\n","        conf_Bm_sum[m] += torch.sum(p_i.flatten() * is_m)\n","\n","    accuracy = correct.mean()\n","\n","  ROCAUC = roc_auc_score(target_all.cpu(), outputs.cpu(), multi_class='ovr')\n","  \n","  ECE = (acc_Bm_sum - conf_Bm_sum).abs().sum()/(n_test)\n","\n","  temp = (acc_Bm_sum - conf_Bm_sum)/Bm\n","  temp[temp!=temp]=0\n","  MCE,_ = temp.abs().max(0)\n","\n","  return accuracy, Loss, ECE, MCE, ROCAUC, output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XK_wJJCIqsWX"},"source":["def test_MoG(net_list, testLoader, batchSize, nSamples,T):\n","  # update ECE\n","  for net in net_list:\n","    net.eval()\n","  accuracy = 0\n","  n_corrects = 0\n","  Loss = 0\n","  num_batches_test = len(testLoader)\n","  n_test = batchSize * num_batches_test\n","  outputs = torch.zeros(n_test, 10).to(DEVICE)\n","  correct = torch.zeros(n_test).to(DEVICE)\n","  target_all = torch.zeros(n_test).to(DEVICE)\n","  n_list = len(net_list)\n","\n","  M = 10\n","  boundary = ((torch.tensor(range(0,M))+1)/10).view(1,-1)\n","  boundary = boundary.repeat(batchSize, 1).to(DEVICE)\n","  \n","  acc_Bm_sum = torch.zeros(M).to(DEVICE)\n","  conf_Bm_sum = torch.zeros(M).to(DEVICE)\n","  Bm = torch.zeros(M).to(DEVICE)\n","  \n","  with torch.no_grad():\n","    for i, (data, target) in enumerate(testLoader):\n","      data, target = data.to(DEVICE), target.to(DEVICE)\n","      target_all[i*batchSize:batchSize*(i+1)] = target\n","      for k, net in enumerate(net_list):\n","        for j in range(nSamples):\n","          output = net(data, sample=True)\n","          outputs[i*batchSize:batchSize*(i+1), :] += output/(nSamples*n_list)\n","          Loss +=  F.nll_loss(torch.log(output), target, size_average=False)/(nSamples*n_list)\n","          # loss is log likelihood\n","          \n","      correct[i*batchSize:batchSize*(i+1)] = (outputs[i*batchSize:batchSize*(i+1), :]).argmax(1).eq(target)\n","      \n","      otemp = outputs[i*batchSize:batchSize*(i+1), :]\n","      p_i,_ = otemp.max(dim=1, keepdims=True)\n","      B = (p_i.le(boundary)*1).argmax(dim=1)\n","      \n","      acc_i = otemp.argmax(1).eq(target)\n","      for m in range(M):\n","        is_m = B.eq(m)\n","        Bm[m] += is_m.sum()\n","        acc_Bm_sum[m] += torch.sum(acc_i * is_m)\n","        conf_Bm_sum[m] += torch.sum(p_i.flatten() * is_m)\n","\n","    accuracy = correct.mean()\n","\n","  ROCAUC = roc_auc_score(target_all.cpu(), outputs.cpu(), multi_class='ovr')\n","  \n","  ECE = (acc_Bm_sum - conf_Bm_sum).abs().sum()/(n_test)\n","\n","  temp = (acc_Bm_sum - conf_Bm_sum)/Bm\n","  temp[temp!=temp]=0\n","  MCE,_ = temp.abs().max(0)\n","\n","  return accuracy, Loss, ECE, MCE, ROCAUC, output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nkzCx11KuzDC"},"source":["def cal_entropy(p):\n","  logP = p.clone()\n","  logP[p==0]=1\n","  logP = torch.log(logP)\n","  return (-logP*p).sum(dim=1)\n","\n","def OOD_test(net, oodLoader, inDis_output, batchSize, nSamples, T, num_class=10):\n","  net.eval()\n","  num_batches_test = len(oodLoader)\n","  n_test = batchSize * num_batches_test\n","  n_inDis = len(inDis_output)\n","\n","  outputs = torch.zeros(n_test, num_class).to(DEVICE)\n","  \n","  target_all = torch.zeros(n_test+n_inDis)\n","  target_all[n_test:] = 1\n","\n","  score1 = torch.zeros(n_test+n_inDis)\n","  score2 = torch.zeros(n_test+n_inDis)\n","\n","  with torch.no_grad():\n","    for i, (data, target) in enumerate(oodLoader):\n","      data = data.to(DEVICE)\n","\n","      for j in range(nSamples):\n","        output = net(data,sample=True)\n","        outputs[i*batch_size:batchSize*(i+1), :] += output/nSamples\n","  entropy = cal_entropy(outputs)\n","  entropy_ave = entropy.mean()\n","  entropy_std = entropy.std()\n","\n","  score1[:n_test],_ = outputs.max(dim=1)\n","  score1[n_test:],_ = inDis_output.max(dim=1)\n","\n","  score2[:n_test] = entropy_ave\n","  score2[n_test:] = cal_entropy(inDis_output).mean()\n","\n","  L2D  = (torch.square(outputs-0.1).sum(dim=1)).mean()\n","  ROCAUC1 = roc_auc_score(target_all, score1, multi_class='ovr', average='weighted')\n","  ROCAUC2 = roc_auc_score(target_all, score2, multi_class='ovr', average='weighted')\n","  return entropy_ave, entropy_std, L2D, ROCAUC1, ROCAUC2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iNuxEbRlqtVj"},"source":["def OOD_test_MoG(net_list, oodLoader, inDis_output, batchSize, nSamples, T, num_class=10):\n","  for net in net_list:\n","    net.eval()\n","  num_batches_test = len(oodLoader)\n","  n_test = batchSize * num_batches_test\n","  n_inDis = len(inDis_output)\n","  n_list = len(net_list)\n","\n","  outputs = torch.zeros(n_test, num_class).to(DEVICE)\n","  \n","  target_all = torch.zeros(n_test+n_inDis)\n","  target_all[n_test:] = 1\n","\n","  score1 = torch.zeros(n_test+n_inDis)\n","  score2 = torch.zeros(n_test+n_inDis)\n","\n","  with torch.no_grad():\n","    for i, (data, target) in enumerate(oodLoader):\n","      data = data.to(DEVICE)\n","      \n","      for k, net in enumerate(net_list):\n","          for j in range(nSamples):\n","            output = net(data,sample=True)\n","            outputs[i*batch_size:batchSize*(i+1), :] += output/(nSamples*n_list)\n","  entropy = cal_entropy(outputs)\n","  entropy_ave = entropy.mean()\n","  entropy_std = entropy.std()\n","\n","  score1[:n_test],_ = outputs.max(dim=1)\n","  score1[n_test:],_ = inDis_output.max(dim=1)\n","\n","  score2[:n_test] = entropy_ave\n","  score2[n_test:] = cal_entropy(inDis_output).mean()\n","\n","  L2D  = (torch.square(outputs-0.1).sum(dim=1)).mean()\n","  ROCAUC1 = roc_auc_score(target_all, score1, multi_class='ovr', average='weighted')\n","  ROCAUC2 = roc_auc_score(target_all, score2, multi_class='ovr', average='weighted')\n","  return entropy_ave, entropy_std, L2D, ROCAUC1, ROCAUC2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XlsZHJ86_28S"},"source":["# Basic setting of Network"]},{"cell_type":"code","metadata":{"id":"CPTawoTn_kdK"},"source":["n_units = 400\n","epochs = 50\n","batch_size = 128\n","# hyperparameter lists\n","T_list = torch.pow(10,-1*torch.tensor(range(0,45,5))/10).to(DEVICE)\n","sigma = torch.tensor(1).to(DEVICE)\n","T=T_list[0]\n","n_samples=5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZyeeWk5NISCp"},"source":["trainAcc = torch.zeros(epochs,).to(DEVICE)\n","trainLoss = torch.zeros(epochs,).to(DEVICE)\n","testAcc = torch.zeros(epochs,).to(DEVICE)\n","testLoss = torch.zeros(epochs,).to(DEVICE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NjFlUW00HALw"},"source":["net = BayesianNetwork(n_units, sigma, T).to(DEVICE)\n","optimizer = optim.Adam(net.parameters())\n","\n","for epoch in range(epochs):\n","  trainAcc[epoch], trainLoss[epoch] = train(net, optimizer, epoch, training_loader, batch_size, 5,T)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zQiYiJvaIL2v"},"source":[""],"execution_count":null,"outputs":[]}]}